{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from segment_anything import SamPredictor, sam_model_registry\n",
    "from segment_anything.utils.transforms import ResizeLongestSide\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision.models import resnet50, ResNet50_Weights, VGG16_Weights\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedicalImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(image_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.images[idx])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "image_dataset = MedicalImageDataset('/media/rohit/mirlproject2/fetal head circumference/Breast_Ultrasound/train/imgs', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate lengths for the splits\n",
    "train_size = int(0.95 * len(image_dataset))\n",
    "val_size = len(image_dataset) - train_size\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, val_dataset = random_split(image_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageEncoderViT(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 1280, kernel_size=(16, 16), stride=(16, 16))\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLPBlock(\n",
       "        (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLPBlock(\n",
       "        (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLPBlock(\n",
       "        (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLPBlock(\n",
       "        (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLPBlock(\n",
       "        (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLPBlock(\n",
       "        (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLPBlock(\n",
       "        (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLPBlock(\n",
       "        (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLPBlock(\n",
       "        (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLPBlock(\n",
       "        (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLPBlock(\n",
       "        (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLPBlock(\n",
       "        (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "    (12): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLPBlock(\n",
       "        (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "    (13): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLPBlock(\n",
       "        (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "    (14): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLPBlock(\n",
       "        (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "    (15): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLPBlock(\n",
       "        (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "    (16): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLPBlock(\n",
       "        (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "    (17): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLPBlock(\n",
       "        (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "    (18): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLPBlock(\n",
       "        (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "    (19): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLPBlock(\n",
       "        (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "    (20): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLPBlock(\n",
       "        (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "    (21): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLPBlock(\n",
       "        (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "    (22): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLPBlock(\n",
       "        (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "    (23): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLPBlock(\n",
       "        (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "    (24): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLPBlock(\n",
       "        (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "    (25): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLPBlock(\n",
       "        (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "    (26): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLPBlock(\n",
       "        (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "    (27): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLPBlock(\n",
       "        (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "    (28): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLPBlock(\n",
       "        (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "    (29): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLPBlock(\n",
       "        (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "    (30): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLPBlock(\n",
       "        (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "    (31): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLPBlock(\n",
       "        (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (neck): Sequential(\n",
       "    (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (1): LayerNorm2d()\n",
       "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (3): LayerNorm2d()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_type = 'vit_h'\n",
    "checkpoint = '/media/rohit/mirlproject2/fetal head circumference/sam_vit_h_4b8939.pth'\n",
    "device = 'cuda'\n",
    "\n",
    "sam_model = sam_model_registry[model_type](checkpoint=checkpoint)\n",
    "sam_model.to(device)\n",
    "\n",
    "sam_encoder = sam_model.image_encoder\n",
    "sam_encoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "class StudentResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StudentResNet, self).__init__()\n",
    "        resnet = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        self.encoder = nn.Sequential(*list(resnet.children())[:-2])  # Use layers except the last two\n",
    "        \n",
    "        # Adjust the channel size from 2048 to 256\n",
    "        self.adjust_channels = nn.Conv2d(2048, 256, kernel_size=1)\n",
    "        \n",
    "        # Add upsampling layers to increase spatial dimensions to [64, 64]\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),  # [14, 14]\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),  # [28, 28]\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),  # [56, 56]\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(size=(64, 64), mode='bilinear', align_corners=False)  # [64, 64]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.adjust_channels(x)\n",
    "        x = self.upsample(x)\n",
    "        return x\n",
    "\n",
    "student_model = StudentResNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "\n",
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        vgg = models.vgg16(weights=VGG16_Weights.DEFAULT).features\n",
    "        self.layers = layers\n",
    "        self.feature_extractor = nn.ModuleList()\n",
    "        start_layer = 0\n",
    "        for end_layer in self.layers:\n",
    "            self.feature_extractor.append(nn.Sequential(*list(vgg.children())[start_layer:end_layer]))\n",
    "            start_layer = end_layer\n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Map of expected input channels at different layers\n",
    "        self.expected_channels = [3, 64, 128, 256, 512]\n",
    "\n",
    "    def forward(self, student_outputs, teacher_outputs):\n",
    "        loss = 0\n",
    "\n",
    "        for i, extractor in enumerate(self.feature_extractor):\n",
    "            expected_channels = self.expected_channels[i]\n",
    "\n",
    "            if student_outputs.size(1) != expected_channels:\n",
    "                student_outputs = nn.Conv2d(student_outputs.size(1), expected_channels, kernel_size=1).to(student_outputs.device)(student_outputs)\n",
    "                teacher_outputs = nn.Conv2d(teacher_outputs.size(1), expected_channels, kernel_size=1).to(teacher_outputs.device)(teacher_outputs)\n",
    "\n",
    "            # Resize outputs to match the expected VGG input size at this layer\n",
    "            target_size = (student_outputs.size(2), student_outputs.size(3))\n",
    "            student_outputs_resized = F.interpolate(student_outputs, size=target_size, mode='bilinear', align_corners=False)\n",
    "            teacher_outputs_resized = F.interpolate(teacher_outputs, size=target_size, mode='bilinear', align_corners=False)\n",
    "\n",
    "            student_features = extractor(student_outputs_resized)\n",
    "            teacher_features = extractor(teacher_outputs_resized)\n",
    "            loss += F.mse_loss(student_features, teacher_features)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedDistillationLoss(nn.Module):\n",
    "    def __init__(self, teacher, layers):\n",
    "        super(CombinedDistillationLoss, self).__init__()\n",
    "        self.teacher = teacher\n",
    "        self.criterion_mse = nn.MSELoss()\n",
    "        self.criterion_perceptual = PerceptualLoss(layers)\n",
    "\n",
    "    def forward(self, student_outputs, images):\n",
    "\n",
    "        tensor_image = images.squeeze(0)\n",
    "        image = tensor_image.permute(1, 2, 0).cpu().numpy()\n",
    "        image = (image * 255).astype(np.uint8)\n",
    "\n",
    "        transform = ResizeLongestSide(sam_model.image_encoder.img_size)\n",
    "        input_image = transform.apply_image(image)\n",
    "        input_image_torch = torch.as_tensor(input_image, device=device)\n",
    "        #input_image_torch = torch.as_tensor(input_image)\n",
    "        transformed_image = input_image_torch.permute(2, 0, 1).contiguous()[None, :, :, :]\n",
    "\n",
    "        input_image = sam_model.preprocess(transformed_image)\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = self.teacher(input_image).detach()\n",
    "\n",
    "        mse_loss = self.criterion_mse(student_outputs, teacher_outputs)\n",
    "        perceptual_loss = self.criterion_perceptual(student_outputs, teacher_outputs)\n",
    "        loss = mse_loss + perceptual_loss\n",
    "        return loss\n",
    "\n",
    "# Instantiate the combined loss function\n",
    "layers = [4, 9, 16, 23, 30]  # Corresponding to relu1_2, relu2_2, relu3_3, relu4_3, relu5_3 in VGG16\n",
    "criterion = CombinedDistillationLoss(sam_encoder, layers).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pth', trace_func=print):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path  # Path to save the checkpoint\n",
    "        self.trace_func = trace_func\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        # Round the validation loss to 5 decimal places\n",
    "        val_loss = round(val_loss, 4)\n",
    "        score = -val_loss  # Convert to a maximization problem\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        \"\"\"Saves model when validation loss decreases.\"\"\"\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.4f} -------> {val_loss:.4f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)  # Save the model state to the specified path\n",
    "        self.val_loss_min = val_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = []\n",
    "validation_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████| 567/567 [05:39<00:00,  1.67batch/s, Train Loss=1.97]\n",
      "Validation 1/100: 100%|██████████| 30/30 [00:17<00:00,  1.74batch/s, Validation Loss=465]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 1.9722788001704679, Validation Loss: 465.3054066936175\n",
      "Saving best model with validation loss-------------------------------------> 465.3054066936175\n",
      "Learning rate: 0.000100\n",
      "Validation loss decreased (inf -------> 465.3054).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100: 100%|██████████| 567/567 [04:48<00:00,  1.97batch/s, Train Loss=1.99]\n",
      "Validation 2/100: 100%|██████████| 30/30 [00:14<00:00,  2.08batch/s, Validation Loss=266]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Training Loss: 1.9851453178143375, Validation Loss: 266.253952729702\n",
      "Saving best model with validation loss-------------------------------------> 266.253952729702\n",
      "Learning rate: 0.000100\n",
      "Validation loss decreased (465.3054 -------> 266.2540).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100: 100%|██████████| 567/567 [04:43<00:00,  2.00batch/s, Train Loss=1.97]\n",
      "Validation 3/100: 100%|██████████| 30/30 [00:14<00:00,  2.08batch/s, Validation Loss=1.2e+3] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Training Loss: 1.973053394802033, Validation Loss: 1199.3242646773656\n",
      "Learning rate: 0.000100\n",
      "EarlyStopping counter: 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100: 100%|██████████| 567/567 [04:43<00:00,  2.00batch/s, Train Loss=1.98]\n",
      "Validation 4/100: 100%|██████████| 30/30 [00:14<00:00,  2.08batch/s, Validation Loss=1.47e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Training Loss: 1.9750590551467169, Validation Loss: 1468.857564496994\n",
      "Learning rate: 0.000100\n",
      "EarlyStopping counter: 2 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100: 100%|██████████| 567/567 [04:43<00:00,  2.00batch/s, Train Loss=1.98]\n",
      "Validation 5/100: 100%|██████████| 30/30 [00:14<00:00,  2.08batch/s, Validation Loss=304]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Training Loss: 1.975257945018681, Validation Loss: 304.2143719991048\n",
      "Learning rate: 0.000100\n",
      "EarlyStopping counter: 3 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100: 100%|██████████| 567/567 [04:43<00:00,  2.00batch/s, Train Loss=1.98]\n",
      "Validation 6/100: 100%|██████████| 30/30 [00:14<00:00,  2.08batch/s, Validation Loss=1.42e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Training Loss: 1.977608647716529, Validation Loss: 1415.5055540919304\n",
      "Learning rate: 0.000100\n",
      "EarlyStopping counter: 4 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100: 100%|██████████| 567/567 [04:43<00:00,  2.00batch/s, Train Loss=1.99]\n",
      "Validation 7/100: 100%|██████████| 30/30 [00:14<00:00,  2.08batch/s, Validation Loss=47.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Training Loss: 1.993566097616098, Validation Loss: 47.66214406490326\n",
      "Saving best model with validation loss-------------------------------------> 47.66214406490326\n",
      "Learning rate: 0.000100\n",
      "Validation loss decreased (266.2540 -------> 47.6621).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/100: 100%|██████████| 567/567 [04:43<00:00,  2.00batch/s, Train Loss=1.99]\n",
      "Validation 8/100: 100%|██████████| 30/30 [00:14<00:00,  2.08batch/s, Validation Loss=1.59e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Training Loss: 1.9908506308913863, Validation Loss: 1589.9158057490984\n",
      "Learning rate: 0.000100\n",
      "EarlyStopping counter: 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/100: 100%|██████████| 567/567 [04:43<00:00,  2.00batch/s, Train Loss=2.01]\n",
      "Validation 9/100: 100%|██████████| 30/30 [00:14<00:00,  2.08batch/s, Validation Loss=14.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Training Loss: 2.013128630908919, Validation Loss: 14.413171581427257\n",
      "Saving best model with validation loss-------------------------------------> 14.413171581427257\n",
      "Learning rate: 0.000100\n",
      "Validation loss decreased (47.6621 -------> 14.4132).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/100: 100%|██████████| 567/567 [04:49<00:00,  1.96batch/s, Train Loss=1.99]\n",
      "Validation 10/100: 100%|██████████| 30/30 [00:16<00:00,  1.78batch/s, Validation Loss=1.04e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Training Loss: 1.9878731366818545, Validation Loss: 1036.3996162931123\n",
      "Learning rate: 0.000100\n",
      "EarlyStopping counter: 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/100: 100%|██████████| 567/567 [05:28<00:00,  1.73batch/s, Train Loss=1.99]\n",
      "Validation 11/100: 100%|██████████| 30/30 [00:16<00:00,  1.86batch/s, Validation Loss=1.25e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Training Loss: 1.9904584260213942, Validation Loss: 1247.7036024570466\n",
      "Learning rate: 0.000100\n",
      "EarlyStopping counter: 2 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/100: 100%|██████████| 567/567 [05:33<00:00,  1.70batch/s, Train Loss=1.96]\n",
      "Validation 12/100: 100%|██████████| 30/30 [00:18<00:00,  1.63batch/s, Validation Loss=828]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Training Loss: 1.962220973859178, Validation Loss: 827.702973083655\n",
      "Learning rate: 0.000100\n",
      "EarlyStopping counter: 3 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/100: 100%|██████████| 567/567 [05:43<00:00,  1.65batch/s, Train Loss=2.01]\n",
      "Validation 13/100: 100%|██████████| 30/30 [00:16<00:00,  1.87batch/s, Validation Loss=1.38e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Training Loss: 2.012135513455359, Validation Loss: 1377.973113612334\n",
      "Learning rate: 0.000100\n",
      "EarlyStopping counter: 4 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/100: 100%|██████████| 567/567 [05:14<00:00,  1.80batch/s, Train Loss=2.01]\n",
      "Validation 14/100: 100%|██████████| 30/30 [00:16<00:00,  1.87batch/s, Validation Loss=834]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Training Loss: 2.0086981247341824, Validation Loss: 834.0124917745591\n",
      "Learning rate: 0.000100\n",
      "EarlyStopping counter: 5 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/100: 100%|██████████| 567/567 [05:15<00:00,  1.80batch/s, Train Loss=1.97]\n",
      "Validation 15/100: 100%|██████████| 30/30 [00:16<00:00,  1.86batch/s, Validation Loss=3.33e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Training Loss: 1.970349653898303, Validation Loss: 3332.34144598643\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Learning rate: 0.000010\n",
      "EarlyStopping counter: 6 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/100: 100%|██████████| 567/567 [05:08<00:00,  1.84batch/s, Train Loss=1.99]\n",
      "Validation 16/100: 100%|██████████| 30/30 [00:14<00:00,  2.08batch/s, Validation Loss=784]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Training Loss: 1.986466644302247, Validation Loss: 784.4091815948486\n",
      "Learning rate: 0.000010\n",
      "EarlyStopping counter: 7 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/100: 100%|██████████| 567/567 [05:27<00:00,  1.73batch/s, Train Loss=1.98]\n",
      "Validation 17/100: 100%|██████████| 30/30 [00:18<00:00,  1.67batch/s, Validation Loss=617]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Training Loss: 1.9831410463524874, Validation Loss: 616.9310113946597\n",
      "Learning rate: 0.000010\n",
      "EarlyStopping counter: 8 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/100: 100%|██████████| 567/567 [05:33<00:00,  1.70batch/s, Train Loss=2]   \n",
      "Validation 18/100: 100%|██████████| 30/30 [00:16<00:00,  1.86batch/s, Validation Loss=1.19e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Training Loss: 1.9977051919520006, Validation Loss: 1191.4472035129866\n",
      "Learning rate: 0.000010\n",
      "EarlyStopping counter: 9 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/100: 100%|██████████| 567/567 [05:55<00:00,  1.60batch/s, Train Loss=2]   \n",
      "Validation 19/100: 100%|██████████| 30/30 [00:15<00:00,  1.89batch/s, Validation Loss=697]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Training Loss: 1.9971257395634996, Validation Loss: 697.3010773181916\n",
      "Learning rate: 0.000010\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Ensure these variables are defined\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(sam_model.mask_decoder.parameters(), lr=1e-4, weight_decay=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "# Early stopping with a specific path to save the best model\n",
    "early_stopping = EarlyStopping(patience=10, verbose=True, path='/media/rohit/mirlproject2/fetal head circumference/Breast_Ultrasound/weights/resnet50_early_stop.pth')\n",
    "\n",
    "# Training loop\n",
    "def train_model(student_model, teacher_model, train_loader, val_loader, criterion, optimizer, num_epochs=100):\n",
    "    best_val_loss = float('inf')\n",
    "    student_model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "        \n",
    "        # Training phase\n",
    "        for images in train_loader_tqdm:\n",
    "            images = images.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            student_outputs = student_model(images)\n",
    "            loss = criterion(student_outputs, images)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            train_loader_tqdm.set_postfix({\"Train Loss\": running_loss / (train_loader_tqdm.n + 1)})\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)  # Average loss per batch\n",
    "        training_loss.append(avg_train_loss)\n",
    "        #print(f'Epoch {epoch+1}, Training Loss: {avg_train_loss}')\n",
    "\n",
    "        val_running_loss = 0.0\n",
    "        student_model.eval()\n",
    "        val_loader_tqdm = tqdm(val_loader, desc=f\"Validation {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "\n",
    "        # Validation phase\n",
    "        with torch.no_grad():\n",
    "            for val_images in val_loader_tqdm:\n",
    "                val_images = val_images.to(device)\n",
    "\n",
    "                student_outputs = student_model(val_images)\n",
    "                loss = criterion(student_outputs, val_images)\n",
    "\n",
    "                val_running_loss += loss.item()\n",
    "                val_loader_tqdm.set_postfix({\"Validation Loss\": val_running_loss / (val_loader_tqdm.n + 1)})\n",
    "\n",
    "        avg_val_loss = val_running_loss / len(val_loader)  # Average loss per batch\n",
    "        validation_loss.append(avg_val_loss)\n",
    "        print(f'Epoch {epoch+1}, Training Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}')\n",
    "\n",
    "        # Check if the current validation loss is the best we've seen so far\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(student_model.state_dict(), '/media/rohit/mirlproject2/fetal head circumference/Breast_Ultrasound/weights/resnet50_best.pth')\n",
    "            print(f'Saving best model with validation loss-------------------------------------> {best_val_loss}')\n",
    "\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        # Print the current learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Learning rate: {current_lr:.6f}\")\n",
    "        \n",
    "        torch.save(student_model.state_dict(), '//media/rohit/mirlproject2/fetal head circumference/Breast_Ultrasound/weights/resnet50_latest.pth')\n",
    "\n",
    "        # Early stopping\n",
    "        early_stopping(avg_val_loss, student_model)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "        student_model.train()\n",
    "\n",
    "train_model(student_model, sam_encoder, train_loader, val_loader, criterion, optimizer, num_epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the dataset and model (assumed to be already defined)\n",
    "# dataset = ...\n",
    "# student_model = ...\n",
    "# sam_encoder = ...\n",
    "\n",
    "# Set parameters\n",
    "k_folds = 5\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Set up the KFold cross-validator\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "# Set up EarlyStopping\n",
    "early_stopping = EarlyStopping(patience=10, verbose=True, path='resnet50_early_stop.pth')\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = CombinedDistillationLoss(sam_encoder, layers).to(device)\n",
    "optimizer = optim.SGD(student_model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# K-Fold Cross-Validation\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n",
    "    print(f'Fold {fold + 1}/{k_folds}')\n",
    "    \n",
    "    # Sample elements randomly from a given list of indices, no replacement\n",
    "    train_subsampler = Subset(dataset, train_idx)\n",
    "    val_subsampler = Subset(dataset, val_idx)\n",
    "\n",
    "    # Define data loaders for training and validation\n",
    "    train_loader = DataLoader(train_subsampler, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_subsampler, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    student_model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        train_loader_tqdm = tqdm(train_loader, desc=f\"Fold {fold + 1}, Epoch {epoch + 1}/{num_epochs}\", unit=\"batch\")\n",
    "        \n",
    "        # Training phase\n",
    "        for images, _ in train_loader_tqdm:\n",
    "            images = images.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            student_outputs = student_model(images)\n",
    "            loss = criterion(student_outputs, images)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            train_loader_tqdm.set_postfix({\"Train Loss\": running_loss / (train_loader_tqdm.n + 1)})\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)  # Average loss per batch\n",
    "        print(f'Epoch {epoch + 1}, Training Loss: {avg_train_loss}')\n",
    "\n",
    "        val_running_loss = 0.0\n",
    "        student_model.eval()\n",
    "        val_loader_tqdm = tqdm(val_loader, desc=f\"Fold {fold + 1}, Validation {epoch + 1}/{num_epochs}\", unit=\"batch\")\n",
    "\n",
    "        # Validation phase\n",
    "        with torch.no_grad():\n",
    "            for val_images, _ in val_loader_tqdm:\n",
    "                val_images = val_images.to(device)\n",
    "\n",
    "                student_outputs = student_model(val_images)\n",
    "                loss = criterion(student_outputs, val_images)\n",
    "\n",
    "                val_running_loss += loss.item()\n",
    "                val_loader_tqdm.set_postfix({\"Validation Loss\": val_running_loss / (val_loader_tqdm.n + 1)})\n",
    "\n",
    "        avg_val_loss = val_running_loss / len(val_loader)  # Average loss per batch\n",
    "        print(f'Epoch {epoch + 1}, Validation Loss: {avg_val_loss}')\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(student_model.state_dict(), f'resnet50_best_fold{fold + 1}.pth')\n",
    "            print(f'Saving best model with validation loss: {best_val_loss}')\n",
    "\n",
    "        torch.save(student_model.state_dict(), f'resnet50_latest_fold{fold + 1}.pth')\n",
    "\n",
    "        # Early stopping\n",
    "        early_stopping(avg_val_loss, student_model)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "        student_model.train()\n",
    "\n",
    "print('K-Fold Cross-Validation complete.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "head",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
